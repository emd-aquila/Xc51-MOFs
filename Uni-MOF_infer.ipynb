{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNmx8VJOp/rXSS5+xGLsoMh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"cellView":"form","id":"RN0MbLuOp0FP"},"outputs":[],"source":["#@title Install Uni-MOF and pretrained weights\n","%%bash\n","cd /content\n","\n","if [ ! -f ENV_READY ]; then\n","\n","    pip3 install rdkit\n","\n","    pip3 install lmdb\n","\n","    pip3 install pymatgen\n","\n","    touch ENV_READY\n","fi\n","\n","UNIMOF_GIT='https://github.com/dptech-corp/Uni-MOF'\n","UNICORE_GIT='https://github.com/dptech-corp/Uni-Core.git'\n","PARAM_URL='https://github.com/dptech-corp/Uni-MOF/releases/download/v0.1/unimof_CoRE_MOFX_DB_finetune_best.pt'\n","\n","if [ ! -f UNIMOF_READY ]; then\n","    git clone -b main ${UNICORE_GIT}\n","    perl -pi -e 's/state = torch\\.load\\(f, map_location=torch\\.device\\(\"cpu\"\\)\\)/state = torch.load(f, map_location=torch.device(\"cpu\"), weights_only=False)/' ./Uni-Core/unicore/checkpoint_utils.py\n","    pip3 install -e ./Uni-Core\n","    git clone -b main ${UNIMOF_GIT}\n","    perl -pi -e 's/unimat\\./unimof./g' ./Uni-MOF/unimof/tasks/*.py\n","    perl -pi -e 's/unimat\\./unimof./g' ./Uni-MOF/unimof/__init__.py\n","    perl -pi -e 's/unimat\\./unimof./g' ./Uni-MOF/unimof/losses/__init__.py\n","    mkdir ./Uni-MOF/weights\n","    wget ${PARAM_URL} -P ./Uni-MOF/weights\n","    touch UNIMOF_READY\n","fi\n","\n","cd /content/Uni-MOF"]},{"cell_type":"code","source":["#@title Download Data\n","%%bash\n","cd /content/Uni-MOF\n","\n","DATA_URL='https://mof.tech.northwestern.edu/Datasets/CoREMOF%202019-10%201021%20acs%20jced%209b00835-all-mofdb-version%3Adc8a0295db.zip'\n","DATA_NAME='CoRE_MOF_2019'\n","\n","if [ ! -f DATA_DOWNLOADED ]; then\n","\n","    mkdir data\n","\n","    wget ${DATA_URL} -O ./data/${DATA_NAME}.zip\n","\n","    unzip ./data/${DATA_NAME}.zip -d ./data/${DATA_NAME}\n","\n","    touch DATA_DOWNLOADED\n","fi\n"],"metadata":{"collapsed":true,"cellView":"form","id":"PM4W9DDDs78J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Convert json data to csv\n","%cd /content/Uni-MOF\n","\n","import json\n","import os\n","import pandas as pd\n","\n","gas_dic = {1:\"CH4\", 2:\"CO2\", 3:\"Ar\", 4:\"Kr\", 5:\"Xe\", 6:\"O2\", 7:\"N2\"}\n","\n","inv_gas_dic = {v: k for k, v in gas_dic.items()}\n","\n","GAS2ATTR = {\n","    \"CH4\":[0.295589,0.165132,0.251511019,-0.61518,0.026952,0.25887781],\n","    \"CO2\":[1.475242,1.475921,1.620478155,0.086439,1.976795,1.69928074],\n","    \"Ar\":[-0.11632,0.294448,0.1914686,-0.01667,-0.07999,-0.1631478],\n","    \"Kr\":[0.48802,0.602454,0.215485568,1.084671,0.415991,0.39885917],\n","    \"Xe\":[1.324657,0.751519,0.233498293,2.276323,1.12122,1.18462811],\n","    \"O2\":[-0.08095,0.37909,0.335570404,-0.61626,-0.5363,-0.1130181],\n","    \"He\":[-1.66617,-1.88746,-2.15618995,-0.9173,-1.36413,-1.6042445],\n","    \"N2\":[-0.37636,-0.3968,0.41962979,-0.31495,-0.40022,-0.3355659],\n","    \"H2\":[-1.34371,-1.3843,-1.11145188,-0.96708,-1.16031,-1.3256695],\n","}\n","\n","directory = './data/CoRE_MOF_2019'\n","\n","data_rows = []\n","\n","for filename in os.listdir(directory):\n","    if filename.endswith(\".json\"):\n","        filepath = os.path.join(directory, filename)\n","        try:\n","            with open(filepath, 'r') as f:\n","                data = json.load(f)\n","                name = data['name']\n","                for iso in data['isotherms']:\n","                    temperature = iso['temperature']\n","                    adsorbates_dict = {ads['InChIKey']:ads['formula'] for ads in iso['adsorbates']}\n","                    for iso_data in iso['isotherm_data']:\n","                        pressure = iso_data['pressure']\n","                        species_data = iso_data['species_data']\n","                        if len(species_data) > 1:\n","                            continue #skip multiple species data for now\n","                        for species in species_data:\n","                            gas_formula = adsorbates_dict[species['InChIKey']]\n","                            if gas_formula not in inv_gas_dic:\n","                                continue\n","                            gas = inv_gas_dic[gas_formula]\n","                            gas_attr = GAS2ATTR[gas_formula]\n","                            loading = species['adsorption']\n","\n","                            data_rows.append({\n","                            'name': name,\n","                            'gas-name': gas,\n","                            'gas-CriticalTemp': gas_attr[0],\n","                            'gas-CriticalPressure': gas_attr[1],\n","                            'gas-AcentricFactor': gas_attr[2],\n","                            'gas-MolecularWeight': gas_attr[3],\n","                            'gas-MeltPoint': gas_attr[4],\n","                            'gas-BoilingPoint': gas_attr[5],\n","                            'temperature': temperature,\n","                            'pressure': pressure,\n","                            'loading_[cm^3(STP)/gr(framework)]_abs_num': loading, #not sure if loading is always given in the same units but leaving as is for now\n","                            })\n","\n","        except json.JSONDecodeError:\n","            print(f\"Error: Could not decode JSON in {filename}\")\n","        except FileNotFoundError:\n","            print(f\"Error: File not found: {filename}\")\n","\n","# Convert list of dicts to DataFrame\n","df = pd.DataFrame(data_rows)\n","\n","# Export to CSV\n","df.to_csv(\"./data/data.csv\", index=True)\n","print(\"Exported parsed data to data.csv\")"],"metadata":{"cellView":"form","id":"FMi-zaha3GaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Preprocess Data\n","from pymatgen.core import Structure\n","from pymatgen.transformations.standard_transformations import ConventionalCellTransformation\n","from pymatgen.symmetry.analyzer import SpacegroupAnalyzer\n","from pymatgen.io.cif import CifParser\n","from multiprocessing import Process, Queue, Pool\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import lmdb\n","import sys\n","import glob\n","import os\n","import re\n","from sklearn.preprocessing import MinMaxScaler\n","import itertools\n","import pdb\n","import random\n","\n","def normalize_atoms(atom):\n","    return re.sub(\"\\d+\", \"\", atom)\n","\n","def transform(cif_path):\n","    max_tolerance = 100\n","    s = CifParser(cif_path, occupancy_tolerance=max_tolerance)\n","    trans = ConventionalCellTransformation()\n","    s_trans = trans.apply_transformation(s.get_structures()[0])\n","    return s_trans\n","\n","def cif_parser(cif_path, primitive=False):\n","    \"\"\"\n","    Parser for single cif file\n","    \"\"\"\n","    try:\n","        s = Structure.from_file(cif_path, primitive=primitive)\n","    except:\n","        s = transform(cif_path)\n","    id = cif_path.split('/')[-1][:-4]\n","    lattice = s.lattice\n","    abc = lattice.abc # lattice vectors\n","    angles = lattice.angles # lattice angles\n","    volume = lattice.volume # lattice volume\n","    lattice_matrix = lattice.matrix # lattice 3x3 matrix\n","\n","    df = s.as_dataframe()\n","    atoms = df['Species'].astype(str).map(normalize_atoms).tolist()\n","    coordinates = df[['x', 'y', 'z']].values.astype(np.float32)\n","    abc_coordinates = df[['a', 'b', 'c']].values.astype(np.float32)\n","    assert len(atoms) == coordinates.shape[0]\n","    assert len(atoms) == abc_coordinates.shape[0]\n","\n","    return {'ID':id,\n","            'atoms':atoms,\n","            'coordinates':coordinates,\n","            'abc':abc,\n","            'angles':angles,\n","            'volume':volume,\n","            'lattice_matrix':lattice_matrix,\n","            'abc_coordinates':abc_coordinates,\n","            }\n","\n","def single_parser(content):\n","    dir_path = './data/CoRE_MOF_2019'\n","    cif_name, gas, gas_attr, temperature, pressure, targets, task_name = content\n","    cif_path = os.path.join(dir_path, cif_name+'.cif')\n","    if os.path.exists(cif_path):\n","        data = cif_parser(cif_path, primitive=False)\n","        data['gas'] = np.array(gas, dtype=np.int32)\n","        data['gas_attr'] = gas_attr.astype(np.float32)\n","        data['temperature'] = np.array(temperature, dtype=np.float32)\n","        data['pressure'] = np.array(np.log10(pressure), dtype=np.float32)\n","        data['target'] = np.array(targets, dtype=np.float32)\n","        data['task_name'] = task_name\n","        return pickle.dumps(data, protocol=-1)\n","    else:\n","        print(f'{cif_path} does not exit!')\n","        return None\n","\n","def get_data(path):\n","    data = pd.read_csv(path)\n","    cif_names = 'name'\n","    gas = 'gas-name'\n","    gas_attr = ['gas-CriticalTemp', 'gas-CriticalPressure', 'gas-AcentricFactor', 'gas-MolecularWeight', 'gas-MeltPoint', 'gas-BoilingPoint']\n","    temperature = 'temperature'\n","    pressure = 'pressure'\n","    columns = 'loading_[cm^3(STP)/gr(framework)]_abs_num'\n","    data['task_name'] = data[cif_names].astype(str) + '#' + data[gas].astype(str) + '#' + data[temperature].astype(str) + '#' + data[pressure].astype(str)\n","\n","    # print mean and std\n","    value_log1p = np.log1p(data[columns])\n","    _mean,_std = value_log1p.mean(), value_log1p.std()\n","    print(f'mean and std of target values are: {_mean}, {_std}')\n","\n","    return [(item[0], item[1], item[2], item[3], item[4], item[5], item[6]) for item in zip(data[cif_names], data[gas], data[gas_attr].values, data[temperature], data[pressure], data[columns], data['task_name'])]\n","\n","# split the database into train, validation and test set according to gases\n","def train_valid_test_split(data, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1):\n","    np.random.seed(42)\n","    id_list = [item[1] for item in data] ##gas_id\n","    unique_id_list = list(set(id_list))\n","    unique_id_list = np.random.permutation(unique_id_list)\n","    print(f'length of data is {len(data)}')\n","    print(f'length of unique_id_list is {len(unique_id_list)}')\n","\n","    gas_dic = {1:\"CH4\", 2:\"CO2\", 3:\"Ar\", 4:\"Kr\", 5:\"Xe\", 6:\"O2\", 7:\"N2\"}\n","    gas_list = [1,2,3,4,5,6,7]\n","\n","    print(\"*******************************\")\n","\n","    test_mat_id = 6\n","    test_id_list = np.array([test_mat_id])\n","    print(\"test_id_list:\", gas_dic[test_mat_id])\n","\n","    gas_list.remove(test_mat_id)\n","    valid_mat_id = random.sample(gas_list,1)\n","    valid_id_list = np.array([valid_mat_id])\n","    print(\"valid_id_list:\", gas_dic[valid_mat_id[0]])\n","\n","    gas_list.remove(valid_mat_id[0])\n","    train_id_list = np.array(gas_list)\n","    print(\"train_id_list:\", train_id_list)\n","\n","    train_data = [item for item in data if item[1] in train_id_list]\n","    valid_data = [item for item in data if item[1] in valid_id_list]\n","    test_data = [item for item in data if item[1] in test_id_list]\n","\n","    print(\"*******************************\")\n","    print(f'train_len:{len(train_data)}')\n","    print(f'valid_len:{len(valid_data)}')\n","    print(f'test_len:{len(test_data)}')\n","\n","    return train_data, valid_data, test_data\n","\n","def rand_test_split(data, num_samples):\n","    np.random.seed(10)\n","    np.random.shuffle(data)\n","    test_data = data[:num_samples]\n","    return [], [], test_data\n","\n","def write_lmdb(inpath='./', outpath='./', nthreads=40):\n","    data = get_data(inpath)\n","    #train_data, valid_data, test_data = train_valid_test_split(data)\n","    train_data, valid_data, test_data = rand_test_split(data, 16)\n","    print(len(train_data), len(valid_data), len(test_data))\n","    for name, content in [ ('train.lmdb', train_data),\n","                            ('valid.lmdb', valid_data),\n","                            ('test.lmdb', test_data) ]:\n","        outputfilename = os.path.join(outpath, name)\n","        os.makedirs(os.path.dirname(outputfilename), exist_ok=True)\n","        try:\n","            os.remove(outputfilename)\n","        except:\n","            pass\n","        env_new = lmdb.open(\n","            outputfilename,\n","            subdir=False,\n","            readonly=False,\n","            lock=False,\n","            readahead=False,\n","            meminit=False,\n","            max_readers=1,\n","            map_size=int(100e9),\n","        )\n","        txn_write = env_new.begin(write=True)\n","        with Pool(nthreads) as pool:\n","            i = 0\n","            for inner_output in tqdm(pool.imap(single_parser, content), total=len(content)):\n","                if inner_output is not None:\n","                    txn_write.put(f'{i}'.encode(\"ascii\"), inner_output)\n","                    i += 1\n","                    if i % 1000 == 0:\n","                        txn_write.commit()\n","                        txn_write = env_new.begin(write=True)\n","            print('{} process {} lines'.format(name, i))\n","            txn_write.commit()\n","            env_new.close()\n","\n","inpath = './data/data.csv' # replace to your data path\n","outpath = './data/CoRE_DB' # replace to your out path\n","!mkdir -p $outpath\n","write_lmdb(inpath=inpath, outpath=outpath, nthreads=8)\n"],"metadata":{"cellView":"form","id":"lM3PcDCOxnCk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cp /content/Uni-MOF/unimof/infer.py /content/Uni-MOF/infer.py\n","%cp /content/Uni-MOF/examples/mof/dict.txt /content/Uni-MOF/data/dict.txt\n","!mkdir -p /content/Uni-MOF/results"],"metadata":{"id":"Xg4oy1DVTt49"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import subprocess\n","\n","cmd = [\n","    \"python\", \"infer.py\", \"./data\",\n","    \"--user-dir\", \"./unimof\",\n","    \"--path\", \"./weights/unimof_CoRE_MOFX_DB_finetune_best.pt\",\n","    \"--task-name\", \"CoRE_DB\",\n","    \"--valid-subset\", \"test\",\n","    \"--num-workers\", \"0\",\n","    \"--task\", \"unimof_v2\",\n","    \"--arch\", \"unimof_v2\",\n","    \"--loss\", \"mof_v2_mse\",\n","    \"--batch-size\", \"4\",\n","    \"--seed\", \"1\",\n","    \"--fp16\",\n","    \"--fp16-init-scale\", \"4\",\n","    \"--fp16-scale-window\", \"256\",\n","    \"--num-classes\", \"1\",\n","    \"--remove-hydrogen\",\n","    \"--results-path\", \"results\",\n","    \"--log-interval\", \"1\",\n","]\n","\n","process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\n","for line in process.stdout:\n","    print(line, end=\"\", flush=True)  # prints output live\n"],"metadata":{"id":"W0IEuXfF8q-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from collections import defaultdict\n","import pickle\n","import torch\n","import matplotlib.pyplot as plt\n","\n","# Load the data\n","with open('./results/weights_test.out.pkl', 'rb') as f:\n","    results = pickle.load(f)\n","\n","# Extract predictions, targets, and names\n","all_preds = []\n","all_targets = []\n","all_names = []\n","\n","for entry in results:\n","    preds = entry['predict'].cpu().numpy().flatten()\n","    targets = entry['target'].cpu().numpy().flatten()\n","    names = entry['task_name']\n","\n","    all_preds.extend(preds)\n","    all_targets.extend(targets)\n","    all_names.extend(names)\n","\n","\n","# Mapping from gas ID to gas name\n","gas_dic = {1: \"CH4\", 2: \"CO2\", 3: \"Ar\", 4: \"Kr\", 5: \"Xe\", 6: \"O2\", 7: \"N2\"}\n","\n","# Group predictions by gas name\n","gas_to_points = defaultdict(list)\n","\n","annotations = []\n","\n","for pred, target, name in zip(all_preds, all_targets, all_names):\n","    parts = name.split('#')\n","    gas_id = int(parts[1])\n","    temp = parts[2]\n","    pressure = parts[3]\n","    gas_name = gas_dic.get(gas_id, f\"Unknown({gas_id})\")\n","\n","    gas_to_points[gas_name].append((target, pred))\n","    annotations.append((target, pred, gas_name, temp, pressure))\n","\n","# Plotting\n","plt.figure(figsize=(10, 10))\n","colors = plt.cm.tab10.colors\n","gas_names = sorted(gas_to_points.keys())\n","color_map = {gas: colors[i % len(colors)] for i, gas in enumerate(gas_names)}\n","\n","for gas_name in gas_names:\n","    points = gas_to_points[gas_name]\n","    t, p = zip(*points)\n","    plt.scatter(t, p, label=gas_name, color=color_map[gas_name], alpha=0.7)\n","\n","# Annotations\n","for t, p, gas, temp, pressure in annotations:\n","    plt.annotate(f'{temp}K, {pressure}Pa', (t, p), textcoords=\"offset points\", xytext=(0,5), ha='center', fontsize=8, alpha=0.6)\n","\n","# Diagonal line\n","min_val = min(all_targets + all_preds)\n","max_val = max(all_targets + all_preds)\n","plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Ideal (y = x)')\n","\n","plt.xlabel('Target')\n","plt.ylabel('Predicted')\n","plt.title('Predicted vs Target (Gas Type + Temp/Pressure)')\n","plt.legend(title=\"Gas\")\n","plt.grid(True)\n","plt.axis('equal')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"450-3z9WgeNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import shutil\n","import os\n","\n","# Define source and destination paths\n","source_dir = \"/content\"\n","dest_dir = '/content/drive/My Drive/Uni-MOF_inference_test'\n","\n","# Define folders or file patterns to exclude\n","excluded_names = ['weights','logs','data','results', '__pycache__', 'drive', 'sample_data']  # Add any others\n","\n","def ignore_func(dir, files):\n","    ignored = []\n","    for name in files:\n","        full_path = os.path.join(dir, name)\n","        for excl in excluded_names:\n","            if name == excl or full_path.startswith(os.path.join(source_dir, excl)):\n","                ignored.append(name)\n","    return ignored\n","\n","# Perform the copy\n","shutil.copytree(source_dir, dest_dir, ignore=ignore_func, dirs_exist_ok=True)"],"metadata":{"id":"5J26jspPm9jX"},"execution_count":null,"outputs":[]}]}